# Generative Models

This repository contains structured notes on **generative models**, focusing on approaches based on **stochastic differential equations (SDEs)** and **ordinary differential equations (ODEs)**. The material develops intuition starting from random walks and stochastic processes and connects them to modern generative modeling techniques such as diffusion models, score matching, Poisson Flow Generative Models, and continuous flow models.

The emphasis is on **conceptual understanding and mathematical structure**, rather than implementation details.

---

## Table of Contents

- Introduction  
- Stochastic Processes  
- Stochastic Differential Equations  
- Ito’s Lemma  
- Fokker–Planck and Kolmogorov Equations  
- Reversing Diffusion  
- Diffusion Models  
- Score Matching  
- Poisson Flow Generative Models  
- Continuous Flow Models  

---

## 1. Introduction

Generative models aim to learn a data distribution and generate new samples from it. In these notes, we study two broad (non-exhaustive) families of generative models:

- **SDE-based models**, which rely on stochastic dynamics and noise
- **ODE-based models**, which rely on deterministic flows

Both frameworks describe how probability distributions evolve over time and provide principled ways to generate samples.

---

## 2. Stochastic Processes

The discussion begins with **discrete random walks**, where a particle moves randomly at each time step. Key observations include:

- The expected position remains zero
- Variance grows linearly with time
- Increments over disjoint time intervals are independent

As the step size goes to zero and the number of steps goes to infinity, the random walk converges to **Brownian motion**, a continuous-time stochastic process with Gaussian increments.

---

## 3. Stochastic Differential Equations (SDEs)

An SDE combines two components:

- A **drift term**, representing deterministic motion
- A **diffusion term**, representing random fluctuations

SDEs describe the evolution of systems under uncertainty and form the foundation of diffusion-based generative models.

---

## 4. Ito’s Lemma

Ito’s Lemma is the stochastic counterpart of the chain rule. It explains how a function of a stochastic variable evolves over time.

A key takeaway is that second-order terms in the stochastic increment contribute at first order in time. This phenomenon has no analogue in ordinary calculus and is central to stochastic analysis.

---

## 5. Fokker–Planck and Kolmogorov Equations

Instead of tracking individual stochastic trajectories, one can study the evolution of **probability densities**.

- The **Fokker–Planck equation** (also called the forward Kolmogorov equation) describes how probability densities evolve forward in time.
- The **backward Kolmogorov equation** describes how transition probabilities evolve backward in time.

These equations connect microscopic dynamics (SDEs) with macroscopic probability flow.

---

## 6. Reversing a Diffusion Process

Diffusion processes gradually destroy information by adding noise. However, if the probability density at each time is known, it is possible to define a **reverse-time dynamics**.

The reverse process depends on the gradient of the log-probability density, often referred to as the **score function**. Learning this score function enables generative sampling by reversing the diffusion.

---

## 7. Diffusion Models

Diffusion models operate in two phases:

1. **Forward process**: data is gradually corrupted by noise until it becomes indistinguishable from pure noise
2. **Reverse process**: noise is gradually removed to generate new samples

The reverse process is learned using neural networks and relies on estimating the score of noisy data at different time steps.

---

## 8. Score Matching

Score matching is a technique for learning the gradient of the log-density directly, without explicitly modeling the density itself.

Key advantages:

- Avoids computing intractable normalization constants
- Learning the score uniquely determines the distribution

Training involves adding noise to data at random times and training a neural network to predict the corresponding score.

---

## 9. Poisson Flow Generative Models (PFGM)

Poisson Flow Generative Models are **ODE-based** generative models inspired by electrostatics.

Core ideas:

- Data points are treated as electric charges
- The data distribution induces an electric field
- Samples are generated by flowing along this field

### Avoiding Mode Collapse

Pure backward flow causes samples to collapse to a point. PFGM avoids this by:

- Introducing an extra dimension
- Starting samples from a high-dimensional surface
- Flowing deterministically back to the data space

This guarantees full coverage of the data distribution.

---

## 10. Continuous Flow Models

Continuous flow models learn a **deterministic vector field** that transforms a simple base distribution into the data distribution.

Key properties:

- Deterministic sampling
- Exact likelihood computation
- Continuous-time transformations

A neural network parameterizes the flow, and both data and probability density evolve jointly under an ordinary differential equation.

---
